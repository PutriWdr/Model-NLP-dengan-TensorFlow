# -*- coding: utf-8 -*-
"""Membuat Model NLP dengan TensorFlow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16sjBQsL0zcwdzg_ea71dr8EbKymdV8na
"""

# install kaggle 
!pip install -q kaggle

"""Nama: Putri Wulandari

Kelas: Machine Learning dan Front End

Proyek Pertama : Membuat Model NLP dengan TensorFlow

# NLP for News Multiclass Categorization
"""

# upload file kaggle.json
from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!ls ~/.kaggle

# test dataset list
!kaggle datasets list

# download dataset, choose 'copy api command' from kaggle dataset
!kaggle datasets download -d hgultekin/bbcnewsarchive

# unzip
!mkdir bbcnewsarchive
!unzip bbcnewsarchive.zip -d bbcnewsarchive
!ls bbcnewsarchive

# import pandas
import pandas as pd

# load dataset
dataku= pd.read_csv('bbcnewsarchive/bbc-news-data.csv', sep='\t')
dataku.head(10)

# columns
dataku.columns

# total data
dataku.shape

# data info
dataku.info()

# kategori
dataku.category.value_counts()

dataku.isnull()

# Find Missing Values in list Dataset
dataku.isnull().sum()

#View Total Number of Missing Value
#Total Number of Missing NA
dataku.isnull().sum().sum()

# delete columns (unused column)
dataku_baru = dataku.drop(columns=['filename'])
dataku_baru

# One-hot encoding

categorynews = pd.get_dummies(dataku.category)
dataku_new = pd.concat([dataku, categorynews], axis=1)
dataku_new = dataku_new.drop(columns='category')
dataku_new

# import and download package
import nltk, os, re, string
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet as wn

nltk.download('wordnet')
nltk.download('stopwords')

# lower-case all characters
dataku_baru.title = dataku_baru.title.apply(lambda x: x.lower())
dataku_baru.content = dataku_baru.content.apply(lambda x: x.lower())

# removing functuation
def cleaner(data):
    return(data.translate(str.maketrans('','', string.punctuation)))
    dataku_baru.title = dataku_baru.title.apply(lambda x: cleaner(x))
    dataku_baru.content = dataku_baru.content.apply(lambda x: lem(x))

## lematization
lemmatizer = WordNetLemmatizer()

def lem(data):
    pos_dict = {'N': wn.NOUN, 'V': wn.VERB, 'J': wn.ADJ, 'R': wn.ADV}
    return(' '.join([lemmatizer.lemmatize(w,pos_dict.get(t, wn.NOUN)) for w,t in nltk.pos_tag(data.split())]))
    dataku_baru.title = dataku_baru.title.apply(lambda x: lem(x))
    datakku_baru.content = dataku_baru.content.apply(lambda x: lem(x))

# removing number
def rem_numbers(data):
    return re.sub('[0-9]+','',data)
    dataku_baru['title'].apply(rem_numbers)
    dataku_baru['content'].apply(rem_numbers)

# removing stopword
st_words = stopwords.words()
def stopword(data):
    return(' '.join([w for w in data.split() if w not in st_words ]))
    dataku_baru.title = dataku_baru.title.apply(lambda x: stopword(x))
    dataku_baru.content = dataku_baru.content.apply(lambda x: lem(x))

# view data after cleansing
dataku_baru.head(10)

# data category one-hot-encoding
category = pd.get_dummies(dataku_baru.category)
dataku_Baru = pd.concat([dataku_baru, category], axis=1)
dataku_Baru = dataku_Baru.drop(columns='category')
dataku_Baru.head(10)

textBaru = dataku_Baru['title'].values + '' + dataku_Baru['content'].values
labelBaru = dataku_Baru[['business', 'entertainment', 'politics', 'sport', 'tech']].values

textBaru

labelBaru

from sklearn.model_selection import train_test_split
textBaru_latih, textBaru_test, labelBaru_latih, labelBaru_test = train_test_split(textBaru, labelBaru, test_size=0.2)

# Tokenizer
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words= 5000, oov_token='x')
tokenizer.fit_on_texts(textBaru_latih) 
tokenizer.fit_on_texts(textBaru_test)
 
sekuens_train = tokenizer.texts_to_sequences(textBaru_latih)
sekuens_test = tokenizer.texts_to_sequences(textBaru_test)
 
padded_latih = pad_sequences(sekuens_train)
padded_test = pad_sequences(sekuens_test)

import tensorflow as tf
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=64),
    tf.keras.layers.LSTM(128),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(5, activation='softmax')
])
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
model.summary()

# callback
class NewsCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.90 and logs.get('val_accuracy')>0.90):
      self.model.stop_training = True
      print("\nHentikan Trainingnya Sekarang , sudah melewati batas  > 90%!")
callbacks = NewsCallback()

num_epochs = 30
history = model.fit(padded_latih, labelBaru_latih, epochs=num_epochs, 
                    validation_data=(padded_test, labelBaru_test), verbose=1,callbacks=[callbacks])

import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Akurasi Model NLP ')
plt.ylabel('accuracy NLP')
plt.xlabel('epoch NLP')
plt.legend(['training', 'testing'], loc='upper left')
plt.show()

import matplotlib.pyplot as plt
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Akurasi Model NLP ')
plt.ylabel('accuracy NLP')
plt.xlabel('epoch NLP')
plt.legend(['training', 'testing'], loc='upper left')
plt.show()

# Conffusion Matriks 
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np

# Defining list with labels
labels = ['bussiness','entertaiment','politics','sport','tech']

# Check point
# Showing labels
print(labels)

# Generating Numpy array with True classes' indexes
y_true = np.random.randint(low=0, high=5, size=100, dtype=int)

# Check point
# Shwoing array
print(y_true)

# Calculating number of samples for every class
# Iterating all classes' indexes in 'y_true' array
# Using Numpy function 'unique'
# Returning sorted unique elements and their frequencies
classesIndexes, classesFrequency = np.unique(y_true, return_counts=True)


# Printing frequency (number of samples) for every class
print('classes indexes:' , classesIndexes)
print('\n')
print('classes frequency:', classesFrequency)

# Commented out IPython magic to ensure Python compatibility.
# Magic function that renders the figure in a jupyter notebook
# instead of displaying a figure object
# %matplotlib inline


# Setting default size of the plot
plt.rcParams['figure.figsize'] = (10.0, 7.0)


# Plotting histogram of 3 classes with their number of samples
# Defining a figure object 
figure = plt.figure()

# Plotting Bar chart
plt.bar(classesIndexes, classesFrequency, align='center', alpha=0.6)


# Giving name to Y axis
plt.ylabel('Class frequency', fontsize=16)


# Giving names to every Bar along X axis
plt.xticks(classesIndexes, labels, fontsize=16)


# Giving name to the plot
plt.title('Histogram', fontsize=20)


# Saving the plot
figure.savefig('histogram.png', transparent=True, dpi=500)


# Showing the plot
plt.show()

# Making copy of array with True classes' indexes
y_predicted = np.copy(y_true)

# Choosing randomly 25% of classes to be changed
ii = np.random.randint(low=0, high=len(y_true), size=int(0.25 * len(y_true)), dtype=int)


# Check point
# Showing chosen indexes
print(ii)

# Iterating chosen indexes and replacing them with other classes' indexes
for i in ii:
    # Generating new class index
    y_predicted[i] = np.random.randint(low=0, high=3, dtype=int)
    
    
    # Check point
    # Showing difference between True classes' indexes and Predicted ones
    print('index = {0:2d}, True class => {1}, {2} <= Predicted class'.
          format(i, y_true[i], y_predicted[i]))

# Confusion Matrix is a two dimensional matrix that visualizes the performance,
# and makes it easy to see confusion between classes,
# by providing a picture of interrelation

# Each row represents a number of actual, True class
# Each column represents a number of predicted class


# Computing Confusion Matrix to evaluate accuracy of classification
c_m = confusion_matrix(y_true, y_predicted)

# Showing Confusion Matrix in form of 2D Numpy array
print(c_m)

# Commented out IPython magic to ensure Python compatibility.
# Magic function that renders the figure in a jupyter notebook
# instead of displaying a figure object
# %matplotlib inline


# Setting default size of the plot
# Setting default fontsize used in the plot
plt.rcParams['figure.figsize'] = (10.0, 9.0)
plt.rcParams['font.size'] = 20


# Implementing visualization of Confusion Matrix
display_c_m = ConfusionMatrixDisplay(c_m, display_labels=labels)


# Plotting Confusion Matrix
# Setting colour map to be used
display_c_m.plot(cmap='OrRd', xticks_rotation=25)
# Other possible options for colour map are:
# 'autumn_r', 'Blues', 'cool', 'Greens', 'Greys', 'PuRd', 'copper_r'


# Setting fontsize for xticks and yticks
plt.xticks(fontsize=15)
plt.yticks(fontsize=15)


# Giving name to the plot
plt.title('Confusion Matrix', fontsize=24)


# Saving plot
plt.savefig('confusion_matrix.png', transparent=True, dpi=500)


# Showing the plot
plt.show()

# Showing the main classification metrics

print(classification_report(y_true, y_predicted))